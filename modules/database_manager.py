# web_scraper/database_manager.py
import sqlite3
import datetime
from supabase import create_client, Client
from supabase.lib.client_options import ClientOptions
from postgrest.exceptions import APIError as SupabaseAPIError
from config_manager import ENV_FILE

from logger import regular, maintenance, debug, error as log_error, critical, warning
from config_manager import get_config, load_config
from utils import generate_unique_id

# --- Database Schema ---
DB_TABLE_NAME = "scraped_pages" # Use a constant for the table name

# SQLite schema definition
SQLITE_TABLE_SCHEMA = f"""
CREATE TABLE IF NOT EXISTS {DB_TABLE_NAME} (
    id INTEGER PRIMARY KEY AUTOINCREMENT,      -- Auto-incrementing ID for local DB
    capture_uuid TEXT UNIQUE NOT NULL,         -- Unique ID for this specific capture instance
    url TEXT NOT NULL,                         -- The URL that was scraped
    scrape_timestamp TEXT NOT NULL,            -- Timestamp of when the scrape occurred (ISO 8601 format)
    scrape_type TEXT NOT NULL,                 -- Type of scrape, e.g., 'primary', 'backup', 'manual'
    html_content TEXT NOT NULL,                -- The full HTML content of the page
    version INTEGER DEFAULT 1                  -- Version number for the scrape data structure (future use)
);
"""
# Note: For Supabase (PostgreSQL), the schema is similar but defined in Supabase Studio.
# Supabase typically uses 'id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY'
# 'scrape_timestamp' would be 'TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL'
# 'capture_uuid' would be 'UUID PRIMARY KEY DEFAULT gen_random_uuid()' or 'TEXT UNIQUE NOT NULL'
# We will map our Python data to the Supabase table structure.

# --- SQLite Connection Management ---
_local_db_connection: sqlite3.Connection | None = None

def get_local_db_connection() -> sqlite3.Connection | None:
    """
    Establishes and returns a SQLite connection using the path from config.
    If a connection already exists, it's returned.
    """
    global _local_db_connection
    if _local_db_connection is None:
        config = get_config() # Ensures config is loaded
        db_path = config.get("LOCAL_DB_PATH", "default_scraper.db")
        try:
            # `check_same_thread=False` is needed if other threads might use this connection,
            # common in scheduled tasks or web apps. For this script, it might not be strictly
            # necessary if scheduling is simple, but it's safer.
            _local_db_connection = sqlite3.connect(db_path, check_same_thread=False)
            _local_db_connection.row_factory = sqlite3.Row # Access columns by name
            maintenance(f"Successfully connected to local SQLite database: {db_path}")
        except sqlite3.Error as e:
            critical(f"CRITICAL: Error connecting to SQLite database at {db_path}", error_obj=e)
            _local_db_connection = None # Ensure it's None if connection failed
    return _local_db_connection

def close_local_db_connection():
    """Closes the SQLite connection if it's open."""
    global _local_db_connection
    if _local_db_connection:
        try:
            _local_db_connection.close()
            _local_db_connection = None
            maintenance("Closed local SQLite database connection.")
        except sqlite3.Error as e:
            log_error("Error closing SQLite database connection", error_obj=e)

# --- Supabase Client Management ---
_supabase_client: Client | None = None

def get_supabase_client() -> Client | None:
    """
    Initializes and returns the Supabase client if credentials are configured.
    If a client instance already exists, it's returned.
    """
    global _supabase_client
    if _supabase_client is None:
        config = get_config() # Ensures config is loaded
        supabase_url = config.get("SUPABASE_URL")
        supabase_key = config.get("SUPABASE_KEY")

        if not supabase_url or not supabase_key:
            debug("Supabase URL or Key not configured. Supabase client will not be initialized.")
            return None
        
        try:
            # ClientOptions can be used for more advanced settings like timeouts, retries
            options = ClientOptions(
                # postgrest_client_timeout=10, # Example: timeout for PostgREST calls
                # storage_client_timeout=60   # Example: timeout for storage calls
            )
            _supabase_client = create_client(supabase_url, supabase_key, options=options)
            maintenance("Successfully initialized Supabase client.")
            # You could add a quick ping or test query here to confirm connectivity
            # e.g., try fetching a small, known piece of data or schema info.
        except Exception as e:
            critical("CRITICAL: Error initializing Supabase client. Check URL/Key and network.", error_obj=e)
            _supabase_client = None
    return _supabase_client

def initialize_databases():
    """
    Ensures the necessary tables are created in the local SQLite database.
    For Supabase, it verifies table accessibility and reminds to create it via Supabase Studio if missing.
    """
    regular("Initializing databases...")
    
    # Initialize SQLite
    conn = get_local_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.executescript(SQLITE_TABLE_SCHEMA) # Use executescript for multi-statement SQL
            conn.commit()
            regular(f"SQLite table '{DB_TABLE_NAME}' ensured/created successfully.")
        except sqlite3.Error as e:
            log_error(f"Error creating/ensuring SQLite table '{DB_TABLE_NAME}'", error_obj=e)
    else:
        log_error("Failed to initialize SQLite database: No connection could be established.")

    # Verify Supabase table (creation is manual via Supabase Studio)
    sb_client = get_supabase_client()
    if sb_client:
        try:
            # Attempt a simple select to check if the table exists and is accessible.
            # Selecting 'capture_uuid' as it's a key column. Limit 0 means we don't fetch data.
            response = sb_client.table(DB_TABLE_NAME).select("capture_uuid", count="exact").limit(0).execute()
            
            # supabase-py v1.x returns response.data and response.count
            # supabase-py v2.x might have slightly different response structure for errors.
            # Check for explicit error in response if API provides it.
            # For Supabase, a common error for a non-existent table is a PostgrestError with code PGRST100
            # or message like 'relation "public.scraped_pages" does not exist'.
            
            # Based on supabase-py v1.x:
            if hasattr(response, 'error') and response.error: # Check for explicit error attribute
                 # This error handling might need adjustment based on specific Supabase client version
                if isinstance(response.error, dict) and response.error.get('message', '').includes('does not exist'):
                     warning(f"Supabase table '{DB_TABLE_NAME}' does not appear to exist. "
                             f"Please create it in your Supabase project dashboard (SQL Editor).")
                else:
                     log_error(f"Error when checking Supabase table '{DB_TABLE_NAME}'. Response error: {response.error}")
            elif response.count is not None: # If count is available, table exists
                regular(f"Supabase table '{DB_TABLE_NAME}' is accessible. Found {response.count or 0} existing records (checked with limit 0).")
            else: # Fallback if response structure is unexpected
                debug(f"Supabase table '{DB_TABLE_NAME}' check response: {response}. Assuming accessible if no explicit error.")

        except SupabaseAPIError as e: # Catch specific Supabase API errors
            if "relation" in str(e).lower() and "does not exist" in str(e).lower():
                warning(f"Supabase table '{DB_TABLE_NAME}' does not exist. "
                        f"Please create it via the Supabase dashboard (SQL Editor). "
                        f"Expected schema elements: id (auto PK), capture_uuid (TEXT/UUID UNIQUE), url (TEXT), "
                        f"scrape_timestamp (TIMESTAMPTZ), scrape_type (TEXT), html_content (TEXT), version (INTEGER).")
            else:
                log_error(f"Supabase API error while checking table '{DB_TABLE_NAME}'. Ensure it's created and accessible.", error_obj=e)
        except Exception as e:
            log_error(f"Unexpected error while checking Supabase table '{DB_TABLE_NAME}'.", error_obj=e)
    else:
        debug("Supabase not configured. Skipping Supabase table check.")
    
    maintenance("Database initialization/verification process complete.")


def save_scrape_data(url: str, html_content: str, scrape_type: str, version: int = 1) -> str | None:
    """
    Saves the scraped HTML data to both SQLite and Supabase (if configured).

    Args:
        url (str): The URL that was scraped.
        html_content (str): The HTML content of the page.
        scrape_type (str): 'primary', 'backup', or 'manual'.
        version (int): Version number for this data structure.

    Returns:
        str | None: The capture_uuid if save was successful at least locally, else None.
    """
    if not html_content:
        log_error(f"No HTML content provided for URL: {url}. Skipping save operation.")
        return None

    capture_id = generate_unique_id() # Generate a unique ID for this capture
    # Use UTC for timestamps to avoid timezone issues, store as ISO 8601 string.
    timestamp_utc = datetime.datetime.now(datetime.timezone.utc)
    timestamp_iso = timestamp_utc.isoformat()

    data_to_insert = {
        "capture_uuid": capture_id,
        "url": url,
        "scrape_timestamp": timestamp_iso,
        "scrape_type": scrape_type,
        "html_content": html_content,
        "version": version
    }
    
    maintenance(f"Preparing to save scrape data for {url} (Capture UUID: {capture_id}, Type: {scrape_type})")
    saved_locally = False
    saved_to_supabase = False

    # Save to SQLite
    conn = get_local_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.execute(f"""
                INSERT INTO {DB_TABLE_NAME} (capture_uuid, url, scrape_timestamp, scrape_type, html_content, version)
                VALUES (:capture_uuid, :url, :scrape_timestamp, :scrape_type, :html_content, :version)
            """, data_to_insert) # Using named placeholders
            conn.commit()
            regular(f"Successfully saved scrape for {url} to SQLite. Rows affected: {cursor.rowcount}")
            saved_locally = True
        except sqlite3.IntegrityError as e: # E.g. UNIQUE constraint failed for capture_uuid
            log_error(f"SQLite IntegrityError for {url} (UUID: {capture_id}). This should not happen with UUIDs.", error_obj=e)
        except sqlite3.Error as e:
            log_error(f"Error saving scrape for {url} to SQLite", error_obj=e)
    else:
        log_error("Cannot save to SQLite: No database connection available.")

    # Save to Supabase
    sb_client = get_supabase_client()
    if sb_client:
        try:
            # Supabase client maps Python dict keys to table column names.
            # Ensure your Supabase table has columns: capture_uuid, url, scrape_timestamp, scrape_type, html_content, version
            # Supabase 'id' column is typically auto-generated (SERIAL or IDENTITY) and should not be in insert data.
            response = sb_client.table(DB_TABLE_NAME).insert(data_to_insert).execute()
            
            # Check response for success (supabase-py v1.x and v2.x differ slightly)
            # For v1.x, successful inserts usually return data in response.data
            # For v2.x, it might be an empty list or specific success indicators.
            # Always check for an error attribute first.
            if hasattr(response, 'error') and response.error:
                log_error(f"Error saving scrape for {url} to Supabase: {response.error}", error_obj=Exception(str(response.error)))
            elif response.data: # Check if data is returned (common in v1.x)
                regular(f"Successfully saved scrape for {url} to Supabase. Response data: {response.data}")
                saved_to_supabase = True
            else: # No explicit error and no data (might be success in v2.x or if returning='minimal')
                maintenance(f"Supabase insert for {url} completed. Response did not contain data or explicit error. Verify in Supabase.")
                # Assuming success if no error, but this might need refinement based on exact Supabase client behavior.
                saved_to_supabase = True 

        except SupabaseAPIError as e:
            log_error(f"Supabase API error saving scrape for {url} to Supabase", error_obj=e)
        except Exception as e:
            log_error(f"Unexpected error saving scrape for {url} to Supabase", error_obj=e)
    else:
        debug("Supabase not configured. Skipping save to Supabase.")

    if saved_locally or saved_to_supabase:
        return capture_id
    return None


if __name__ == '__main__':
    # Example Usage & Testing
    # Ensure you have a .env file with SUPABASE_URL and SUPABASE_KEY for Supabase tests
    # or comment out Supabase related parts if not testing Supabase.
    # from config_manager import save_config_value # To set up for test
    
    # Load config directly to ensure logger level is set from .env if defined
    load_config() # This will also set the global log level
    # If you want to override for testing:
    # from logger import set_log_level
    # set_log_level("DEBUG")

    regular("Starting database_manager test script...")

    # --- Setup for testing (Optional: configure Supabase via .env) ---
    # print("Simulating .env setup for Supabase (if not already set in your .env):")
    # config = get_config()
    # if not config.get("SUPABASE_URL"):
    #     print("WARNING: SUPABASE_URL not set. Supabase tests will be skipped or may fail.")
    # if not config.get("SUPABASE_KEY"):
    #     print("WARNING: SUPABASE_KEY not set. Supabase tests will be skipped or may fail.")
    
    print("\n--- Initializing Databases (creates tables if they don't exist) ---")
    initialize_databases()

    print("\n--- Testing Save Operation ---")
    test_url_1 = "http://example.com/testpage1"
    test_html_1 = "<html><body><h1>Test HTML 1</h1><p>This is a primary test.</p></body></html>"
    
    capture_uuid_1 = save_scrape_data(test_url_1, test_html_1, "primary", version=1)
    if capture_uuid_1:
        print(f"Save operation for {test_url_1} reported success with UUID: {capture_uuid_1}")
    else:
        print(f"Save operation for {test_url_1} failed or did not save to any database.")

    test_url_2 = "http://example.com/testpage2"
    test_html_2 = "<html><body><h1>Test HTML 2</h1><p>This is a backup test. Version 2.</p></body></html>"
    capture_uuid_2 = save_scrape_data(test_url_2, test_html_2, "backup", version=2)
    if capture_uuid_2:
        print(f"Save operation for {test_url_2} reported success with UUID: {capture_uuid_2}")
    else:
        print(f"Save operation for {test_url_2} failed or did not save to any database.")

    print("\n--- Testing Save Operation with Empty HTML (should be skipped) ---")
    test_url_empty = "http://empty.example.com/page"
    capture_uuid_empty = save_scrape_data(test_url_empty, "", "primary")
    if capture_uuid_empty:
        print(f"ERROR: Save operation for empty HTML for {test_url_empty} unexpectedly returned a UUID: {capture_uuid_empty}")
    else:
        print(f"Correctly skipped save for empty HTML for {test_url_empty}.")


    print("\n--- Verifying SQLite Data (Manual Query) ---")
    conn = get_local_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            # Fetch a few recent records
            cursor.execute(f"SELECT capture_uuid, url, scrape_type, scrape_timestamp, version FROM {DB_TABLE_NAME} ORDER BY id DESC LIMIT 5")
            rows = cursor.fetchall()
            if rows:
                print(f"Found {len(rows)} recent rows in SQLite table '{DB_TABLE_NAME}':")
                for row_idx, row_data in enumerate(rows):
                    # Access by column name due to row_factory = sqlite3.Row
                    print(f"  Row {row_idx + 1}: UUID: {row_data['capture_uuid']}, URL: {row_data['url']}, Type: {row_data['scrape_type']}, Timestamp: {row_data['scrape_timestamp']}, Version: {row_data['version']}")
            else:
                print(f"No rows found in SQLite table '{DB_TABLE_NAME}'. This might be an issue if saves were expected.")
        except sqlite3.Error as e:
            log_error("Error querying SQLite for verification", error_obj=e)
    else:
        print("Could not connect to SQLite to verify data.")

    print("\n--- Supabase Data Verification ---")
    print("If Supabase is configured, please check your Supabase project dashboard to verify data insertion for the tests above.")
    sb_client_check = get_supabase_client()
    if sb_client_check:
        print("Supabase client is configured. Manual verification in Supabase Studio is recommended.")
    else:
        print("Supabase client is NOT configured. Supabase saves were skipped.")
    
    # Important: Close the local DB connection when the script/application is done.
    close_local_db_connection()
    
    print("\nDatabase manager test script complete.")

